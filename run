#!/usr/bin/env python3.6
"""Commands necessaries to define a run script that will acts as entrypoint for Docker. This script will provide a
common interface to run the application, tests and utils.
"""
import argparse
import asyncio
import fileinput
import io
import json
import os
import shlex
import sys
import time
from typing import List

from aiokafka import AIOKafkaConsumer, AIOKafkaProducer
from avro.io import BinaryDecoder, BinaryEncoder, DatumReader, DatumWriter
from clinner.command import Type as CommandType, command
from clinner.run import Main as BaseMain

from tasks.utils import get_schema

# Binaries
PYTHON = 'python3.6'
COVERAGE = 'coverage'
PROSPECTOR = 'prospector'
HEALTH_CHECK = 'health_check'
PYTEST = 'pytest'

# Constants
DEFAULT_KAFKA_BOOTSTRAP_SERVER = 'kafka:9002'


# Commands
@command(command_type=CommandType.PYTHON)
def app(*args, **kwargs):
    from tasks.flights import FlightsTask

    time.sleep(5)
    FlightsTask().run()


@command(command_type=CommandType.SHELL)
def shell(*args, **kwargs) -> List[List[str]]:
    cmd = shlex.split(f'ipython')
    cmd += args
    return [cmd]


@command(command_type=CommandType.SHELL)
def unit_tests(*args, **kwargs) -> List[List[str]]:
    coverage_erase = shlex.split(f'{COVERAGE} erase')
    tests = [PYTEST] + list(args)
    return [coverage_erase, tests]


@command(command_type=CommandType.SHELL)
def prospector(*args, **kwargs) -> List[List[str]]:
    cmd = [PROSPECTOR]
    cmd += args
    return [cmd]


@command(args=((('topic',), {'help': 'Kafka topic'}),
               (('schema',), {'help': 'Avro schema'}),
               (('-f', '--file'), {'help': 'File to be loaded. In jsonlines format.', 'default': '-'}),
               (('--bootstrap-servers',),
                {'help': 'Kafka bootstrap servers', 'default': DEFAULT_KAFKA_BOOTSTRAP_SERVER}),),
         parser_opts={'help': 'Command line kafka producer'})
def kafka_producer(*args, **kwargs):
    loop = asyncio.get_event_loop()

    schema = get_schema('flights.avpr', kwargs['schema'])

    def serialize(item):
        buffer = io.BytesIO()
        DatumWriter(schema).write(json.loads(item), BinaryEncoder(buffer))
        buffer.seek(0)
        return buffer.read()

    producer = AIOKafkaProducer(loop=loop, bootstrap_servers=kwargs['bootstrap_servers'],
                                value_serializer=serialize, request_timeout_ms=3000, metadata_max_age_ms=3000)

    async def _produce(topic):
        await producer.start()
        try:
            with fileinput.input(files=[kwargs['file']]) as f:
                for raw_message in f:
                    await producer.send_and_wait(topic, raw_message)
        except KeyboardInterrupt:
            pass
        finally:
            await producer.stop()

    loop.run_until_complete(_produce(kwargs['topic']))


@command(args=((('topic',), {'help': 'Kafka topic'}),
               (('schema',), {'help': 'Avro schema'}),
               (('--bootstrap-servers',),
                {'help': 'Kafka bootstrap servers', 'default': DEFAULT_KAFKA_BOOTSTRAP_SERVER}),
               (('-g', '--group-id'), {'help': 'Consumer group ID', 'default': 'group_id'})),
         parser_opts={'help': 'Command line kafka consumer'})
def kafka_consumer(*args, **kwargs):
    loop = asyncio.get_event_loop()

    schema = get_schema('flights.avpr', kwargs['schema'])

    def deserialize(item):
        return str(DatumReader(schema).read(BinaryDecoder(io.BytesIO(item))))

    consumer = AIOKafkaConsumer(kwargs['topic'], loop=loop, bootstrap_servers=kwargs['bootstrap_servers'],
                                group_id=kwargs['group_id'], value_deserializer=deserialize)

    async def _consume():
        await consumer.start()
        try:
            async for msg in consumer:
                print(msg.value)
        except KeyboardInterrupt:
            pass
        finally:
            consumer.stop()

    loop.run_until_complete(_consume())


# Main
class Main(BaseMain):
    commands = (
        'app',
        'shell',
        'unit_tests',
        'prospector',
        'kafka_producer',
        'kafka_consumer',
    )

    def add_arguments(self, parser: argparse.ArgumentParser):
        parser.add_argument('-s', '--settings', default='flights_analyzer.settings:Settings', help='Settings class')

    def inject_app_settings(self):
        os.environ['APP_SETTINGS'] = self.settings


if __name__ == '__main__':
    main = Main()
    sys.exit(main.run())
